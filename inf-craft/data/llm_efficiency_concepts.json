{
  "concepts": {
    "linear_algebra": {
      "id": "linear_algebra",
      "name": "Linear Algebra",
      "description": "Foundation of matrix operations, vector spaces, and linear transformations",
      "difficulty": 1,
      "category": "mathematics",
      "prerequisites": [],
      "leads_to": ["attention_mechanism", "state_space_models", "linear_attention"],
      "properties": {
        "complexity": "O(n³) for matrix multiplication",
        "key_operations": ["matrix_multiplication", "eigendecomposition", "linear_transformations"]
      },
      "resources": ["3Blue1Brown Linear Algebra", "Gilbert Strang Course"]
    },
    "calculus": {
      "id": "calculus",
      "name": "Calculus",
      "description": "Derivatives, integrals, and continuous mathematics",
      "difficulty": 1,
      "category": "mathematics",
      "prerequisites": [],
      "leads_to": ["backpropagation", "state_space_models", "optimization"],
      "properties": {
        "key_concepts": ["derivatives", "chain_rule", "integration"],
        "applications": ["gradient_computation", "continuous_systems"]
      },
      "resources": ["Khan Academy Calculus", "MIT OCW"]
    },
    "probability": {
      "id": "probability",
      "name": "Probability & Statistics",
      "description": "Probability distributions, information theory, and statistical inference",
      "difficulty": 1,
      "category": "mathematics",
      "prerequisites": [],
      "leads_to": ["attention_mechanism", "mixture_of_experts", "routing"],
      "properties": {
        "distributions": ["normal", "exponential", "softmax"],
        "concepts": ["entropy", "kl_divergence", "bayesian_inference"]
      },
      "resources": ["Pattern Recognition and ML by Bishop", "Information Theory by MacKay"]
    },
    "neural_networks": {
      "id": "neural_networks",
      "name": "Neural Network Fundamentals",
      "description": "Feedforward networks, activation functions, and backpropagation",
      "difficulty": 2,
      "category": "deep_learning",
      "prerequisites": ["linear_algebra", "calculus", "probability"],
      "leads_to": ["attention_mechanism", "transformers", "mixture_of_experts"],
      "properties": {
        "activation_functions": ["relu", "sigmoid", "tanh", "gelu"],
        "optimization": ["sgd", "adam", "learning_rate_scheduling"],
        "loss_functions": ["cross_entropy", "mse", "l1_l2_regularization"]
      },
      "resources": ["Deep Learning by Goodfellow", "CS231n Course"]
    },
    "attention_mechanism": {
      "id": "attention_mechanism",
      "name": "Attention Mechanism",
      "description": "Query-Key-Value paradigm for selective information processing",
      "difficulty": 3,
      "category": "transformer",
      "prerequisites": ["neural_networks", "linear_algebra"],
      "leads_to": ["standard_attention", "linear_attention", "sparse_attention"],
      "properties": {
        "formula": "Attention(Q,K,V) = softmax(QK^T/√d)V",
        "complexity": "O(N²d)",
        "memory": "O(N²)",
        "key_components": ["query", "key", "value", "softmax"]
      },
      "resources": ["Attention Is All You Need", "Illustrated Transformer"]
    },
    "standard_attention": {
      "id": "standard_attention",
      "name": "Standard Self-Attention",
      "description": "Full attention computation with quadratic complexity",
      "difficulty": 3,
      "category": "attention",
      "prerequisites": ["attention_mechanism"],
      "leads_to": ["linear_attention", "sparse_attention", "flash_attention"],
      "properties": {
        "complexity": "O(N²d)",
        "memory": "O(N²)",
        "advantages": ["exact_computation", "full_context"],
        "disadvantages": ["quadratic_scaling", "memory_bottleneck"]
      },
      "resources": ["Transformer Paper", "Attention Visualization Tools"]
    },
    "linear_attention": {
      "id": "linear_attention",
      "name": "Linear Attention",
      "description": "Approximate attention with linear complexity using kernel methods",
      "difficulty": 4,
      "category": "efficient_attention",
      "prerequisites": ["standard_attention", "kernel_methods"],
      "leads_to": ["recurrent_linear_attention", "feature_mappings"],
      "properties": {
        "complexity": "O(Nd²)",
        "memory": "O(d²)",
        "formula": "o_t = φ(q_t)Σφ(k_i)^T v_i / φ(q_t)Σφ(k_i)^T",
        "feature_mappings": ["elu_plus_one", "random_features", "polynomial_kernels"]
      },
      "resources": ["Transformers are RNNs", "Efficient Attention Paper"]
    },
    "kernel_methods": {
      "id": "kernel_methods",
      "name": "Kernel Methods",
      "description": "Feature mappings and kernel tricks for linear approximations",
      "difficulty": 3,
      "category": "mathematics",
      "prerequisites": ["linear_algebra", "probability"],
      "leads_to": ["linear_attention", "random_features"],
      "properties": {
        "techniques": ["feature_mappings", "kernel_tricks", "random_projection"],
        "applications": ["softmax_approximation", "linear_complexity"]
      },
      "resources": ["Kernel Methods in ML", "Random Features Paper"]
    },
    "recurrent_linear_attention": {
      "id": "recurrent_linear_attention",
      "name": "Recurrent Linear Attention",
      "description": "Linear attention with recurrent state updates",
      "difficulty": 4,
      "category": "efficient_attention",
      "prerequisites": ["linear_attention", "recurrent_networks"],
      "leads_to": ["state_space_models", "gated_linear_rnn"],
      "properties": {
        "formula": "S_t = S_{t-1} + φ(k_t)^T v_t, o_t = φ(q_t)S_t / φ(q_t)z_t",
        "advantages": ["constant_memory", "streaming_processing"],
        "complexity": "O(d²) per token"
      },
      "resources": ["Linear Attention Paper", "Recurrent Implementation"]
    },
    "state_space_models": {
      "id": "state_space_models",
      "name": "State Space Models (SSMs)",
      "description": "Continuous-time state space representation with linear complexity",
      "difficulty": 4,
      "category": "efficient_architectures",
      "prerequisites": ["calculus", "linear_algebra", "differential_equations"],
      "leads_to": ["s4", "mamba", "mamba2"],
      "properties": {
        "continuous_form": "x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t)",
        "discrete_form": "A = exp(ΔA), B = (ΔA)⁻¹(exp(ΔA) - I) · ΔB",
        "complexity": "O(Nd)",
        "memory": "O(d)"
      },
      "resources": ["S4 Paper", "Mamba Paper", "Control Theory Books"]
    },
    "s4": {
      "id": "s4",
      "name": "S4 (Structured State Spaces)",
      "description": "SSM with HiPPO initialization and structured parameters",
      "difficulty": 5,
      "category": "ssm_variants",
      "prerequisites": ["state_space_models", "hippo_initialization"],
      "leads_to": ["mamba", "mamba2"],
      "properties": {
        "initialization": "HiPPO (High-order Polynomial Projection Operators)",
        "structure": "diagonal + low-rank",
        "advantages": ["long_sequence_modeling", "efficient_training"]
      },
      "resources": ["S4 Paper", "HiPPO Paper"]
    },
    "mamba": {
      "id": "mamba",
      "name": "Mamba",
      "description": "Data-dependent SSM with selective state transitions",
      "difficulty": 5,
      "category": "ssm_variants",
      "prerequisites": ["s4", "selective_mechanisms"],
      "leads_to": ["mamba2", "hybrid_architectures"],
      "properties": {
        "key_innovation": "data_dependent_parameters",
        "selective_mechanism": "learned_gating",
        "complexity": "O(Nd)",
        "advantages": ["selective_processing", "efficient_inference"]
      },
      "resources": ["Mamba Paper", "Mamba Implementation"]
    },
    "mamba2": {
      "id": "mamba2",
      "name": "Mamba2",
      "description": "Scalar state transitions with blockwise computation",
      "difficulty": 5,
      "category": "ssm_variants",
      "prerequisites": ["mamba", "blockwise_computation"],
      "leads_to": ["hybrid_architectures", "hardware_optimization"],
      "properties": {
        "innovation": "scalar_state_transitions",
        "computation": "blockwise_parallel",
        "advantages": ["hardware_efficiency", "scalability"]
      },
      "resources": ["Mamba2 Paper", "Hardware Implementation"]
    },
    "sparse_attention": {
      "id": "sparse_attention",
      "name": "Sparse Attention",
      "description": "Selective attention computation with reduced complexity",
      "difficulty": 4,
      "category": "efficient_attention",
      "prerequisites": ["standard_attention"],
      "leads_to": ["static_sparse", "dynamic_sparse", "local_attention"],
      "properties": {
        "complexity": "O(sNd) where s << N",
        "memory": "O(sN)",
        "approaches": ["static_patterns", "dynamic_patterns", "learned_routing"]
      },
      "resources": ["Sparse Transformer", "Longformer", "BigBird"]
    },
    "static_sparse": {
      "id": "static_sparse",
      "name": "Static Sparse Attention",
      "description": "Predefined, fixed sparsity patterns",
      "difficulty": 4,
      "category": "sparse_attention",
      "prerequisites": ["sparse_attention"],
      "leads_to": ["local_attention", "dilated_attention", "global_attention"],
      "properties": {
        "patterns": ["local", "dilated", "random", "global"],
        "advantages": ["deterministic", "hardware_friendly"],
        "disadvantages": ["fixed_patterns", "limited_adaptability"]
      },
      "resources": ["Sparse Transformer Paper", "Longformer Paper"]
    },
    "dynamic_sparse": {
      "id": "dynamic_sparse",
      "name": "Dynamic Sparse Attention",
      "description": "Content-dependent sparsity patterns",
      "difficulty": 5,
      "category": "sparse_attention",
      "prerequisites": ["sparse_attention", "clustering"],
      "leads_to": ["reformer", "memorizing_transformers"],
      "properties": {
        "strategies": ["clustering", "retrieval", "learned_routing"],
        "advantages": ["adaptive_patterns", "content_aware"],
        "disadvantages": ["computational_overhead", "complexity"]
      },
      "resources": ["Reformer Paper", "Memorizing Transformers"]
    },
    "local_attention": {
      "id": "local_attention",
      "name": "Local Attention",
      "description": "Sliding window attention with local context",
      "difficulty": 4,
      "category": "static_sparse",
      "prerequisites": ["static_sparse"],
      "leads_to": ["longformer", "sliding_window"],
      "properties": {
        "pattern": "sliding_window",
        "complexity": "O(wNd) where w is window size",
        "advantages": ["linear_scaling", "local_context"],
        "disadvantages": ["limited_long_range"]
      },
      "resources": ["Longformer Paper", "Sliding Window Implementation"]
    },
    "dilated_attention": {
      "id": "dilated_attention",
      "name": "Dilated Attention",
      "description": "Exponentially increasing attention spans",
      "difficulty": 4,
      "category": "static_sparse",
      "prerequisites": ["static_sparse"],
      "leads_to": ["sparse_transformer", "multi_scale_attention"],
      "properties": {
        "pattern": "exponential_dilation",
        "advantages": ["long_range_access", "efficient_pattern"],
        "disadvantages": ["fixed_dilation", "limited_flexibility"]
      },
      "resources": ["Sparse Transformer Paper", "Dilated Convolution Literature"]
    },
    "reformer": {
      "id": "reformer",
      "name": "Reformer",
      "description": "LSH-based dynamic sparse attention",
      "difficulty": 5,
      "category": "dynamic_sparse",
      "prerequisites": ["dynamic_sparse", "locality_sensitive_hashing"],
      "leads_to": ["lsh_attention", "bucket_attention"],
      "properties": {
        "technique": "Locality-Sensitive Hashing (LSH)",
        "complexity": "O(N log N)",
        "advantages": ["scalable", "content_aware"],
        "disadvantages": ["approximate", "hash_dependent"]
      },
      "resources": ["Reformer Paper", "LSH Literature"]
    },
    "locality_sensitive_hashing": {
      "id": "locality_sensitive_hashing",
      "name": "Locality-Sensitive Hashing",
      "description": "Similarity-preserving hashing for efficient similarity search",
      "difficulty": 4,
      "category": "algorithms",
      "prerequisites": ["hashing", "similarity_metrics"],
      "leads_to": ["reformer", "approximate_nearest_neighbor"],
      "properties": {
        "principle": "similar_tokens_hash_to_same_bucket",
        "applications": ["similarity_search", "clustering", "attention_approximation"]
      },
      "resources": ["LSH Survey", "Similarity Search Literature"]
    },
    "mixture_of_experts": {
      "id": "mixture_of_experts",
      "name": "Mixture of Experts (MoE)",
      "description": "Conditional computation with multiple specialized experts",
      "difficulty": 4,
      "category": "efficient_architectures",
      "prerequisites": ["neural_networks", "routing"],
      "leads_to": ["token_choice_routing", "expert_choice_routing", "load_balancing"],
      "properties": {
        "formula": "Y = Σ_k Top-k(P) · E_k(X)",
        "routing": "P = Softmax(XW_g + b_g)",
        "advantages": ["scalable_capacity", "conditional_computation"],
        "disadvantages": ["routing_challenges", "load_balancing"]
      },
      "resources": ["GShard Paper", "Switch Transformers", "GLaM Paper"]
    },
    "routing": {
      "id": "routing",
      "name": "Routing Mechanisms",
      "description": "Methods for directing tokens to appropriate experts",
      "difficulty": 4,
      "category": "moe_components",
      "prerequisites": ["mixture_of_experts"],
      "leads_to": ["token_choice_routing", "expert_choice_routing", "adaptive_routing"],
      "properties": {
        "strategies": ["token_choice", "expert_choice", "adaptive"],
        "objectives": ["load_balancing", "expert_utilization", "quality"]
      },
      "resources": ["MoE Routing Papers", "Load Balancing Literature"]
    },
    "token_choice_routing": {
      "id": "token_choice_routing",
      "name": "Token-Choice Routing",
      "description": "Each token selects top-k experts",
      "difficulty": 4,
      "category": "routing",
      "prerequisites": ["routing"],
      "leads_to": ["load_balancing", "expert_specialization"],
      "properties": {
        "mechanism": "tokens_select_experts",
        "advantages": ["token_control", "flexible_assignment"],
        "disadvantages": ["load_imbalance", "expert_underutilization"]
      },
      "resources": ["Switch Transformers", "Token-Choice Analysis"]
    },
    "expert_choice_routing": {
      "id": "expert_choice_routing",
      "name": "Expert-Choice Routing",
      "description": "Each expert selects top-k tokens",
      "difficulty": 4,
      "category": "routing",
      "prerequisites": ["routing"],
      "leads_to": ["load_balancing", "expert_utilization"],
      "properties": {
        "mechanism": "experts_select_tokens",
        "advantages": ["better_load_balancing", "expert_utilization"],
        "disadvantages": ["token_assignment_constraints"]
      },
      "resources": ["Expert-Choice Paper", "Load Balancing Analysis"]
    },
    "load_balancing": {
      "id": "load_balancing",
      "name": "Load Balancing",
      "description": "Ensuring even distribution of work across experts",
      "difficulty": 4,
      "category": "moe_components",
      "prerequisites": ["mixture_of_experts", "routing"],
      "leads_to": ["auxiliary_losses", "expert_utilization"],
      "properties": {
        "loss": "L_aux = (1/N) Σᵢ₌₁ᴺ D_i(X) · G_i(X)",
        "techniques": ["auxiliary_losses", "expert_diversity", "routing_constraints"]
      },
      "resources": ["Load Balancing Papers", "MoE Training Literature"]
    },
    "flash_attention": {
      "id": "flash_attention",
      "name": "FlashAttention",
      "description": "IO-aware attention optimization for memory bandwidth",
      "difficulty": 5,
      "category": "attention_optimization",
      "prerequisites": ["standard_attention", "memory_hierarchy"],
      "leads_to": ["flash_attention2", "flash_attention3"],
      "properties": {
        "optimization": "memory_bandwidth",
        "technique": "tiling_and_online_computation",
        "advantages": ["memory_efficient", "exact_computation"],
        "complexity": "same_as_standard_attention"
      },
      "resources": ["FlashAttention Paper", "Memory Optimization Literature"]
    },
    "memory_hierarchy": {
      "id": "memory_hierarchy",
      "name": "Memory Hierarchy",
      "description": "SRAM, DRAM, and GPU memory organization",
      "difficulty": 3,
      "category": "hardware",
      "prerequisites": ["computer_architecture"],
      "leads_to": ["flash_attention", "io_aware_algorithms"],
      "properties": {
        "levels": ["sram", "dram", "gpu_memory"],
        "characteristics": ["speed", "capacity", "bandwidth"],
        "optimization": "data_locality"
      },
      "resources": ["Computer Architecture Books", "GPU Memory Guides"]
    },
    "quantization": {
      "id": "quantization",
      "name": "Quantization",
      "description": "Reducing precision while maintaining accuracy",
      "difficulty": 4,
      "category": "optimization",
      "prerequisites": ["neural_networks"],
      "leads_to": ["int8_quantization", "fp8_quantization", "mixed_precision"],
      "properties": {
        "techniques": ["post_training", "quantization_aware_training", "mixed_precision"],
        "precisions": ["int8", "fp8", "fp16", "fp32"],
        "advantages": ["memory_efficiency", "speed"],
        "disadvantages": ["accuracy_loss", "training_complexity"]
      },
      "resources": ["Quantization Papers", "PyTorch Quantization"]
    },
    "hybrid_architectures": {
      "id": "hybrid_architectures",
      "name": "Hybrid Architectures",
      "description": "Combining efficient and standard components",
      "difficulty": 5,
      "category": "advanced_architectures",
      "prerequisites": ["standard_attention", "efficient_architectures"],
      "leads_to": ["zamba", "jamba", "hymba"],
      "properties": {
        "approaches": ["inter_layer", "intra_layer", "head_wise", "sequence_wise"],
        "advantages": ["best_of_both_worlds", "flexibility"],
        "disadvantages": ["increased_complexity", "design_challenges"]
      },
      "resources": ["Zamba Paper", "Jamba Paper", "Hybrid Architecture Papers"]
    },
    "zamba": {
      "id": "zamba",
      "name": "Zamba",
      "description": "Mamba backbone with periodic attention layers",
      "difficulty": 5,
      "category": "hybrid_architectures",
      "prerequisites": ["mamba", "hybrid_architectures"],
      "leads_to": ["jamba", "periodic_attention"],
      "properties": {
        "architecture": "mamba_backbone + periodic_attention",
        "advantages": ["efficiency + expressiveness", "long_context"],
        "design": "inter_layer_hybrid"
      },
      "resources": ["Zamba Paper", "Mamba + Attention Literature"]
    },
    "jamba": {
      "id": "jamba",
      "name": "Jamba",
      "description": "Mamba + Attention + MoE combination",
      "difficulty": 5,
      "category": "hybrid_architectures",
      "prerequisites": ["zamba", "mixture_of_experts"],
      "leads_to": ["multi_component_hybrids"],
      "properties": {
        "architecture": "mamba + attention + moe",
        "advantages": ["scalable", "efficient", "expressive"],
        "complexity": "high_design_challenge"
      },
      "resources": ["Jamba Paper", "Multi-Component Architecture Papers"]
    },
    "diffusion_llms": {
      "id": "diffusion_llms",
      "name": "Diffusion LLMs",
      "description": "Non-autoregressive generation using diffusion processes",
      "difficulty": 5,
      "category": "generation_methods",
      "prerequisites": ["diffusion_models", "language_modeling"],
      "leads_to": ["llada", "bd3_lms"],
      "properties": {
        "generation": "non_autoregressive",
        "advantages": ["parallel_decoding", "controllability", "bidirectional_context"],
        "disadvantages": ["training_complexity", "quality_challenges"]
      },
      "resources": ["LLaDA Paper", "Diffusion Models Literature"]
    },
    "diffusion_models": {
      "id": "diffusion_models",
      "name": "Diffusion Models",
      "description": "Generative models using forward and reverse diffusion processes",
      "difficulty": 4,
      "category": "generative_models",
      "prerequisites": ["probability", "markov_processes"],
      "leads_to": ["diffusion_llms", "image_generation"],
      "properties": {
        "process": "forward_noise + reverse_denoising",
        "advantages": ["high_quality", "controllable"],
        "disadvantages": ["slow_generation", "training_complexity"]
      },
      "resources": ["DDPM Paper", "Diffusion Models Survey"]
    },
    "llada": {
      "id": "llada",
      "name": "LLaDA",
      "description": "Large Language Model Diffusion Accelerator",
      "difficulty": 5,
      "category": "diffusion_llms",
      "prerequisites": ["diffusion_llms"],
      "leads_to": ["parallel_language_generation"],
      "properties": {
        "framework": "diffusion_for_language_modeling",
        "objective": "L(θ) = -E_{t,x₀,x_t}[1/t Σᵢ₌₁ᴸ 1[xᵢt = M] log p_θ(xᵢ₀|x_t)]",
        "advantages": ["parallel_generation", "bidirectional_context"]
      },
      "resources": ["LLaDA Paper", "Diffusion Language Modeling"]
    },
    "cross_modal_applications": {
      "id": "cross_modal_applications",
      "name": "Cross-Modal Applications",
      "description": "Extensions beyond text to vision, audio, and multimodal",
      "difficulty": 5,
      "category": "applications",
      "prerequisites": ["efficient_architectures", "multimodal_learning"],
      "leads_to": ["vision_mamba", "audio_ssm", "multimodal_moe"],
      "properties": {
        "modalities": ["vision", "audio", "multimodal"],
        "architectures": ["mamba_backbones", "linear_attention", "moe_scaling"],
        "applications": ["classification", "generation", "understanding"]
      },
      "resources": ["Vision Mamba", "AudioCraft", "Flamingo"]
    },
    "vision_mamba": {
      "id": "vision_mamba",
      "name": "Vision Mamba",
      "description": "Mamba-based architectures for computer vision",
      "difficulty": 5,
      "category": "cross_modal",
      "prerequisites": ["mamba", "computer_vision"],
      "leads_to": ["image_classification", "object_detection", "segmentation"],
      "properties": {
        "tasks": ["classification", "detection", "segmentation"],
        "advantages": ["efficient_processing", "long_context"],
        "architecture": "mamba_backbone"
      },
      "resources": ["Vision Mamba Papers", "Computer Vision Literature"]
    },
    "audio_ssm": {
      "id": "audio_ssm",
      "name": "Audio SSMs",
      "description": "State space models for audio processing",
      "difficulty": 5,
      "category": "cross_modal",
      "prerequisites": ["state_space_models", "audio_processing"],
      "leads_to": ["speech_recognition", "music_generation", "audio_enhancement"],
      "properties": {
        "tasks": ["understanding", "enhancement", "generation"],
        "advantages": ["temporal_modeling", "efficiency"],
        "applications": ["speech", "music", "audio_analysis"]
      },
      "resources": ["Audio SSM Papers", "Audio Processing Literature"]
    },
    "multimodal_moe": {
      "id": "multimodal_moe",
      "name": "Multimodal MoE",
      "description": "Mixture of experts for multimodal models",
      "difficulty": 5,
      "category": "cross_modal",
      "prerequisites": ["mixture_of_experts", "multimodal_learning"],
      "leads_to": ["scalable_multimodal", "unified_generation"],
      "properties": {
        "scaling": "moe_for_multimodal",
        "advantages": ["scalable_capacity", "modality_specialization"],
        "applications": ["vision_language", "audio_visual", "unified_models"]
      },
      "resources": ["Multimodal MoE Papers", "Large Multimodal Models"]
    },
    "complexity_analysis": {
      "id": "complexity_analysis",
      "name": "Complexity Analysis",
      "description": "Theoretical analysis of computational and memory complexity",
      "difficulty": 3,
      "category": "analysis",
      "prerequisites": ["algorithms", "complexity_theory"],
      "leads_to": ["performance_comparison", "scaling_analysis"],
      "properties": {
        "metrics": ["time_complexity", "space_complexity", "memory_usage"],
        "analysis": ["big_o_notation", "empirical_measurement", "theoretical_bounds"]
      },
      "resources": ["Algorithm Analysis", "Complexity Theory"]
    },
    "performance_comparison": {
      "id": "performance_comparison",
      "name": "Performance Comparison",
      "description": "Empirical comparison of different methods",
      "difficulty": 4,
      "category": "analysis",
      "prerequisites": ["complexity_analysis", "benchmarking"],
      "leads_to": ["method_selection", "optimization_decisions"],
      "properties": {
        "metrics": ["speed", "memory", "quality", "scalability"],
        "benchmarks": ["synthetic_tests", "real_world_tasks", "standard_datasets"]
      },
      "resources": ["Benchmark Papers", "Performance Analysis Tools"]
    },
    "hardware_optimization": {
      "id": "hardware_optimization",
      "name": "Hardware Optimization",
      "description": "Algorithm-system-hardware co-design for efficiency",
      "difficulty": 5,
      "category": "optimization",
      "prerequisites": ["hardware_architecture", "algorithm_design"],
      "leads_to": ["custom_hardware", "specialized_accelerators"],
      "properties": {
        "co_design": "algorithm_system_hardware",
        "optimization": ["memory_bandwidth", "compute_efficiency", "energy_efficiency"]
      },
      "resources": ["Hardware Co-design Papers", "Custom Accelerator Literature"]
    },
    "future_directions": {
      "id": "future_directions",
      "name": "Future Directions",
      "description": "Emerging research directions and frontiers",
      "difficulty": 5,
      "category": "research",
      "prerequisites": ["all_concepts"],
      "leads_to": ["infinite_context", "efficient_agents", "continual_learning"],
      "properties": {
        "frontiers": ["infinite_context", "efficient_agents", "multimodal_reasoning", "continual_learning"],
        "challenges": ["scalability", "efficiency", "quality", "adaptability"]
      },
      "resources": ["Research Papers", "Conference Proceedings", "Industry Blogs"]
    }
  },
  "relationships": {
    "prerequisites": {
      "attention_mechanism": ["neural_networks", "linear_algebra"],
      "linear_attention": ["standard_attention", "kernel_methods"],
      "state_space_models": ["calculus", "linear_algebra", "differential_equations"],
      "mamba": ["s4", "selective_mechanisms"],
      "sparse_attention": ["standard_attention"],
      "mixture_of_experts": ["neural_networks", "routing"],
      "hybrid_architectures": ["standard_attention", "efficient_architectures"],
      "diffusion_llms": ["diffusion_models", "language_modeling"],
      "cross_modal_applications": ["efficient_architectures", "multimodal_learning"]
    },
    "leads_to": {
      "linear_algebra": ["attention_mechanism", "state_space_models", "linear_attention"],
      "calculus": ["backpropagation", "state_space_models", "optimization"],
      "probability": ["attention_mechanism", "mixture_of_experts", "routing"],
      "neural_networks": ["attention_mechanism", "transformers", "mixture_of_experts"],
      "attention_mechanism": ["standard_attention", "linear_attention", "sparse_attention"],
      "standard_attention": ["linear_attention", "sparse_attention", "flash_attention"],
      "linear_attention": ["recurrent_linear_attention", "feature_mappings"],
      "state_space_models": ["s4", "mamba", "mamba2"],
      "s4": ["mamba", "mamba2"],
      "mamba": ["mamba2", "hybrid_architectures"],
      "sparse_attention": ["static_sparse", "dynamic_sparse", "local_attention"],
      "static_sparse": ["local_attention", "dilated_attention", "global_attention"],
      "dynamic_sparse": ["reformer", "memorizing_transformers"],
      "mixture_of_experts": ["token_choice_routing", "expert_choice_routing", "load_balancing"],
      "routing": ["token_choice_routing", "expert_choice_routing", "adaptive_routing"],
      "hybrid_architectures": ["zamba", "jamba", "hymba"],
      "diffusion_llms": ["llada", "bd3_lms"],
      "cross_modal_applications": ["vision_mamba", "audio_ssm", "multimodal_moe"]
    }
  },
  "categories": {
    "mathematics": ["linear_algebra", "calculus", "probability", "kernel_methods", "complexity_analysis"],
    "deep_learning": ["neural_networks"],
    "transformer": ["attention_mechanism"],
    "attention": ["standard_attention", "linear_attention", "sparse_attention", "flash_attention"],
    "efficient_attention": ["linear_attention", "recurrent_linear_attention", "sparse_attention"],
    "ssm_variants": ["s4", "mamba", "mamba2"],
    "efficient_architectures": ["state_space_models", "mixture_of_experts"],
    "sparse_attention": ["static_sparse", "dynamic_sparse", "local_attention", "dilated_attention"],
    "static_sparse": ["local_attention", "dilated_attention", "global_attention"],
    "dynamic_sparse": ["reformer", "memorizing_transformers"],
    "algorithms": ["locality_sensitive_hashing"],
    "moe_components": ["routing", "load_balancing"],
    "routing": ["token_choice_routing", "expert_choice_routing"],
    "attention_optimization": ["flash_attention"],
    "hardware": ["memory_hierarchy"],
    "optimization": ["quantization"],
    "advanced_architectures": ["hybrid_architectures"],
    "hybrid_architectures": ["zamba", "jamba", "hymba"],
    "generation_methods": ["diffusion_llms"],
    "generative_models": ["diffusion_models"],
    "diffusion_llms": ["llada", "bd3_lms"],
    "applications": ["cross_modal_applications"],
    "cross_modal": ["vision_mamba", "audio_ssm", "multimodal_moe"],
    "analysis": ["complexity_analysis", "performance_comparison"],
    "research": ["future_directions"]
  },
  "difficulty_levels": {
    "1": ["linear_algebra", "calculus", "probability"],
    "2": ["neural_networks"],
    "3": ["attention_mechanism", "memory_hierarchy", "complexity_analysis"],
    "4": ["linear_attention", "state_space_models", "sparse_attention", "mixture_of_experts", "quantization", "performance_comparison"],
    "5": ["s4", "mamba", "mamba2", "reformer", "flash_attention", "hybrid_architectures", "diffusion_llms", "cross_modal_applications", "hardware_optimization", "future_directions"]
  }
}
